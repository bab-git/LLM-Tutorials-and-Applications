# LLM Applications and Tools

## Overview

The goal of this repository is to provide a collection of applications that can be used with Large Language Models (LLMs). These applications serve as starting points for building advanced LLM products. The material collected here is sourced from public resources, including GitHub repositories and various courses. 

As the technology evolves, some packages and tools may become outdated. However, the effort will be made to update this repository with new tools and technologies whenever possible. At the very least, these notebooks demonstrate the product-level possibilities available for leveraging LLMs.

## Table of Contents
- [Fundamentals](#fundamentals)
- [Retrieval-Augmented Generation (RAG)](#retrieval-augmented-generation-rag)
- [Agents and Tools](#agents-and-tools)
- [Contributing](#contributing)
- [License](#license)

## Fundamentals
The Fundamentals section covers the essential concepts and techniques for working with Large Language Models (LLMs). This section provides a solid foundation for understanding and utilizing LLMs, including topics such as chatbots, conversational agents, chains, prompt templates, and output parsers. By exploring these basics, you will gain the necessary skills to build simple to advanced applications that leverage the power of LLMs for various conversational tasks and QA systems.

This section covers general topics related to QA, chatbots, chains, and other conversational topics with LLMs, ranging from simple to advanced.

- **[Introduction and Basics of Chatbots and Conversational Agents](notebooks/Fundamentals/QA_chatbot.ipynb)**
  - A direct API call from OpenAI to perform chat completions
  - Chat completions with Langchain APIs
  - Using prompt template to create custom prompt
  - Using output parser to extract key information from the completion
  - **Use Case:**
    - Technical support as a chatbot technician.
    - Entity extraction from cusomer travel query.

- **[Using memory to have an Interactive Conversational Chatbot](notebooks/Fundamentals/QA_chatbot_memory.ipynb)**
  - Complete memory recording
  - Window based memory
  - Summary based memory
  - **Use Case:**
    - Technical support
    - Customer support
    - Healthcare consultation 

- **[Using Langchain to generate LLM chains](notebooks/Fundamentals/llm_chains.ipynb)**
  - Introduction to chains
  - LLMChain and Language Execution Chain (LEC)
  - Intermediate outputs in chains
  - **Use Case:**
    - Patients inquiry analysis
    - Patient symptoms categorization
  
- **Comming soon:**
  - Using few shot chains to instruct LLMs
  - Numerical evaluation metrics: BLUE, ROUGE, METEOR, CER


## Retrieval-Augmented Generation (RAG)

Retrieval-Augmented Generation (RAG) is a powerful technique that combines the strengths of information retrieval and natural language generation. By integrating retrieval mechanisms with LLMs, RAG can access and utilize vast amounts of external knowledge, enabling the generation of more accurate, contextually relevant, and informative responses. This approach is particularly useful for tasks requiring up-to-date information, detailed explanations, or specific data points, significantly enhancing the capabilities and performance of LLM-based applications.

This section focuses on techniques and implementations for combining LLMs with retrieval systems to enhance performance.

- **[Question and Answering (QA) over documents using RAG](notebooks/Retrieval_Augmented_Generation/RAG_basic.ipynb)**
  - A simple in memory RAG application using langchain based on a CSV file
  - **Use Case:**
    - Search and summary of clinical trials

- **[RAG application using persistent and cloud-based vector stores](notebooks/Retrieval_Augmented_Generation/RAG_vectorstores.ipynb)**
  - Using a public embedding model from Hugging Face
  - Applying chunking strategy for analyzing large documents
  - Using a persistent vector store from Chroma DB
  - A cloud-based Vector Store from Pinecone
  - **Case Study:**
    - Search and retrieve information from Novartis 2023 annual report.
  
- **[Evaluation of Retreival Augmented Generation using QAGenerator:](notebooks/Retrieval_Augmented_Generation/RAG_evaluation.ipynb)**
  - Using QAGenerator to have ground truth samples for QA-applications
  - Evaluating QA-based RAG using QAGeneratorChain and QAEvalChain
  - **Case Study:** Amazon Product Catalog
  
- **Comming Soon:**
  - Ranking retrieved documents using similarity scores
  - Using UMAP to visualize retrieved documents similarity
  - Window-based, Hierarchical retrieval, and MAP-RERANK to expand input length of RAG pipelines
  - Graph RAG


## Agents and Tools
Large Language Model (LLM) agents are crucial for creating dynamic, interactive, and context-aware applications. 
These agents can handle complex tasks by understanding and generating human-like text, making them invaluable in automating processes, providing intelligent responses, and integrating with various tools and services to perform specific actions. 
By leveraging LLM agents, developers can build robust applications that offer enhanced user experiences and operational efficiencies.  

This section is dedicated to building and managing complex agents, tool integration, and advanced chain techniques.

- **[Agent-based application of Large Language Models](notebooks/Agents/llm_agents_basics.ipynb)**
  - An LLM agent that calls a single python interpreter tool.
  - A ReAct based LLM agent that uses two built-in tools, a calculator and a wikipedia search.
  - User defined agent and its tools

## Contributing

Contributions are welcome! If you have any suggestions, improvements, or new tools and technologies to add, please feel free to submit a pull request or open an issue.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
